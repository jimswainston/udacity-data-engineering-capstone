{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Locate spark on local machine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 10:32:54 WARN Utils: Your hostname, BIN-LP-04283 resolves to a loopback address: 127.0.1.1; using 172.16.5.38 instead (on interface eth2)\n",
      "22/08/22 10:32:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/22 10:32:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Setup SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data object type is <class 'dict'>\n",
      "Data has the following keys dict_keys(['items'])\n",
      "The type of the value corresponding to the item key is <class 'list'>\n",
      "The length of the list contained in the items dict is 5000\n",
      "<class 'dict'>\n",
      "33\n",
      "dict_keys(['URL', 'resource', 'member', 'score', 'created', 'license', 'ISSN', 'container-title', 'issued', 'issue', 'prefix', 'reference-count', 'indexed', 'author', 'DOI', 'is-referenced-by-count', 'published', 'published-print', 'subject', 'published-online', 'content-domain', 'title', 'link', 'source', 'type', 'publisher', 'journal-issue', 'volume', 'references-count', 'issn-type', 'deposited', 'page', 'short-container-title'])\n",
      "Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy\n",
      "Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy\n",
      "Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy\n",
      "Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy\n",
      "Research Department, Aventis Pasteur, Campus Merieux, 69280 Marcy l'Etoile, France\n",
      "From the Institut für Physiologie der Universität Regensburg (K.-H.H., A.K., P.S.), Regensburg, Germany; and Klinik für Kinder und Jugendliche (E.S.), Erlangen, Germany.\n",
      "From the Institut für Physiologie der Universität Regensburg (K.-H.H., A.K., P.S.), Regensburg, Germany; and Klinik für Kinder und Jugendliche (E.S.), Erlangen, Germany.\n",
      "From the Institut für Physiologie der Universität Regensburg (K.-H.H., A.K., P.S.), Regensburg, Germany; and Klinik für Kinder und Jugendliche (E.S.), Erlangen, Germany.\n",
      "From the Institut für Physiologie der Universität Regensburg (K.-H.H., A.K., P.S.), Regensburg, Germany; and Klinik für Kinder und Jugendliche (E.S.), Erlangen, Germany.\n",
      "Organisch-Chemisches Institut, Westfälische Wilhelms-Universität Münster, Corrensstrasse 40, D-48149 Münster, Germany\n",
      "Organisch-Chemisches Institut, Westfälische Wilhelms-Universität Münster, Corrensstrasse 40, D-48149 Münster, Germany\n",
      "Facultad de Química, Pontificia Universidad Católica de Chile, Casilla 306, Santiago 22, Chile\n",
      "Facultad de Química, Pontificia Universidad Católica de Chile, Casilla 306, Santiago 22, Chile\n",
      "Facultad de Química, Pontificia Universidad Católica de Chile, Casilla 306, Santiago 22, Chile\n",
      "<i>Leeves:</i> School of Economics, University of Queensland, St. Lucia, Brisbane, QLD 4072, Australia. E‐mail\n",
      "UPRES-A-7035, Département de Mathématiques, Universit de METZ, BP 80794, F-57012 Metz Cedex, France\n",
      "23 Allée des Oeillets, F-57160 Moulins les Metz, France\n",
      "a Laboratory Services Division , Canadian Food Inspection Agency , Ottawa , Ont , K1A 0C6 , Canada\n",
      "a Laboratory Services Division , Canadian Food Inspection Agency , Ottawa , Ont , K1A 0C6 , Canada\n",
      "Sony Corporation, Research Center, Yokohama 240-0036, Japan\n",
      "Sony Corporation, Research Center, Yokohama 240-0036, Japan\n",
      "Sony Corporation, Research Center, Yokohama 240-0036, Japan\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n",
      "Comparative and Ecological Phytochemistry Department, Institute of Botany, University of Vienna, Rennweg 14, A-1030 Wien, Austria, Department of Botany, Faculty of Science, Kasetsart University, Bangkok 10900, Thailand, and Institute of Organic Chemistry, University of Vienna, Währingerstrasse 38, A-1090 Wien, Austria\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'author'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=21'>22</a>\u001b[0m i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data[\u001b[39m\"\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=23'>24</a>\u001b[0m     \u001b[39m#i = i +1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=24'>25</a>\u001b[0m     \u001b[39m#print(\"article number\" + str(i) + item[\"type\"])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=25'>26</a>\u001b[0m     \u001b[39mfor\u001b[39;00m author \u001b[39min\u001b[39;00m item[\u001b[39m\"\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=26'>27</a>\u001b[0m         \u001b[39mfor\u001b[39;00m affiliation \u001b[39min\u001b[39;00m author[\u001b[39m\"\u001b[39m\u001b[39maffiliation\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=27'>28</a>\u001b[0m             \u001b[39mprint\u001b[39m(affiliation[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'author'"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "#Test file path /home/jswainston/Downloads/0.json.gz\n",
    "#df = spark.read.json(\"/home/jswainston/Downloads/April2022CrossrefPublicDataFile/0.json.gz\",multiLine=True)\n",
    "#df.printSchema()\n",
    "#df.show()\n",
    "\n",
    "\n",
    "with gzip.open(\"/home/jswainston/Downloads/April2022CrossrefPublicDataFile/0.json.gz\", 'r') as fin:        # 4. gzip\n",
    "    json_bytes = fin.read()                      # 3. bytes (i.e. UTF-8)\n",
    "\n",
    "json_str = json_bytes.decode('utf-8')            # 2. string (i.e. JSON)\n",
    "data = json.loads(json_str)                      # 1. data\n",
    "print(\"data object type is \" + str(type(data)))\n",
    "\n",
    "print(\"Data has the following keys \" + str(data.keys()))\n",
    "print(\"The type of the value corresponding to the item key is \" + str(type(data[\"items\"])))\n",
    "print(\"The length of the list contained in the items dict is \" + str(len(data[\"items\"])))\n",
    "print(type(data[\"items\"][0]))\n",
    "print(len((data[\"items\"][0])))\n",
    "print(data[\"items\"][0].keys())\n",
    "\n",
    "#access author affiliations\n",
    "for item in data[\"items\"]:\n",
    "    for author in item[\"author\"]:\n",
    "        for affiliation in author[\"affiliation\"]:\n",
    "            print(affiliation[\"name\"]) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 17:46:41 WARN Utils: Your hostname, LP568 resolves to a loopback address: 127.0.1.1; using 192.168.0.10 instead (on interface wifi0)\n",
      "22/07/14 17:46:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/jswainston/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jswainston/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jswainston/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d8c0d073-c516-4bd8-8768-c0fdbbafa30d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      "downloading https://repos.spark-packages.org/saurfang/spark-sas7bdat/2.0.0-s_2.11/spark-sas7bdat-2.0.0-s_2.11.jar ...\n",
      "\t[SUCCESSFUL ] saurfang#spark-sas7bdat;2.0.0-s_2.11!spark-sas7bdat.jar (1454ms)\n",
      "downloading https://repo1.maven.org/maven2/com/epam/parso/2.0.8/parso-2.0.8.jar ...\n",
      "\t[SUCCESSFUL ] com.epam#parso;2.0.8!parso.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api-scala_2.11/2.7/log4j-api-scala_2.11-2.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.logging.log4j#log4j-api-scala_2.11;2.7!log4j-api-scala_2.11.jar (188ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (65ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.8!scala-reflect.jar (650ms)\n",
      ":: resolution report :: resolve 6207ms :: artifacts dl 2540ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d8c0d073-c516-4bd8-8768-c0fdbbafa30d\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (4883kB/38ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 17:46:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: scala/Product$class\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:48)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:39)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: scala.Product$class\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=2'>3</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39m\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=3'>4</a>\u001b[0m config(\u001b[39m\"\u001b[39m\u001b[39mspark.jars.repositories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttps://repos.spark-packages.org/\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=4'>5</a>\u001b[0m config(\u001b[39m\"\u001b[39m\u001b[39mspark.jars.packages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaurfang:spark-sas7bdat:2.0.0-s_2.11\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39m\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=5'>6</a>\u001b[0m enableHiveSupport()\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m df_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.github.saurfang.sas.spark\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m df_spark\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    178\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: scala/Product$class\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:48)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:39)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: scala.Product$class\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
