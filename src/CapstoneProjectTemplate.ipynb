{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is an exploratory project to understand how metadata produced from the production of academic articles could be modelled and stored using a star schema design to enable efficient querying for analytics purposes.  The primary data source is the Crossref Public Data file (https://www.crossref.org/blog/2022-public-data-file-of-more-than-134-million-metadata-records-now-available/) which is a collection of more than 134 million metadata records. These records are produced from metadata submitted to Crossref by academic publishers as part of their publication process.  \n",
    "\n",
    "Additionally I have integrated GDP (gross domestic product) data to allow end users to try an answer questions such as how macroeconomic factors may influence the publishing environment. For example, is there a correlation between high GDP output and the number of academic documents produced?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I have written an ETL program that extracts information from journal article metadata and loads them into a star schema style database. In addition I have also loaded GDP data. The data from both sources has been integrated by using conformed dimensions for year and country. This allows end users to join facts about academic articles to GDP amounts. \n",
    "For the purposes of this project I have used a locally hosted PostgreSQL database, a Jupyter notebook and Python as the programming language of choice. \n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The Crossref Public Data file which is the source of this data was downloaded from here: https://academictorrents.com/details/4dcfdf804775f2d92b7a030305fa0350ebef6f3e\n",
    "The full dataset includes 26,810 compressed JSON files. An example of one of these JSON files can be viewed here: https://raw.githubusercontent.com/jimswainston/udacity-data-engineering-capstone/main/data/Crossref/0.json. The type of information that is included is when a document was published, who it was published by, who the authors are, which institutions the authors are affiliated with at time of publication, the subject areas that the content covers and information about the journal that the article is contained in.\n",
    "\n",
    "The GDP data was sourced from the World Bank https://data.worldbank.org/indicator/NY.GDP.MKTP.CD. It is a CSV file that contains tabular data on the US$ GDP amounts for all countries that it keeps records for. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "One of the key pieces of information of interest is who authored a paper and the institution they were affiliated with. The institutional affiliation usually includes the country which allows us to understand the geography of where academic research is being produced. Unfortunately the affiliation isn't always present. See below JSON snippet for example. Missing affiliations need to be handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"author\": [\n",
    "        {\n",
    "          \"given\": \"Richard\",\n",
    "          \"family\": \"Arkaifie\",\n",
    "          \"sequence\": \"first\",\n",
    "          \"affiliation\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another core issue is that no standard taxonomy is used for entering country information for an affiliation. The affiliation is typically provided by the document author when they submit their article to a publisher. You could have different values being used to refer to the same geoographic region. For example, USA and North America. The other issue with the country information is that it is unstructured and is a free text part of the affiliation. See the below snippet as an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"author\": [\n",
    "        {\n",
    "          \"given\": \"Dale D.\",\n",
    "          \"family\": \"Tang\",\n",
    "          \"sequence\": \"first\",\n",
    "          \"affiliation\": [\n",
    "            {\n",
    "              \"name\": \"Department of Cellular and Integrative Physiology, Indiana University School of Medicine, 635 Barnhill Drive, Indianapolis, IN 46202, USA\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI 10.1149/1.1392467\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "#Country information is extracted and cleansed using the following Python method that is found in the dataExtractUtils.py module\n",
    "#This is an example of it working\n",
    "\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import gzip\n",
    "import json\n",
    "import dataExtractUtils as de\n",
    "import uuid\n",
    "import constants\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "with gzip.open(\"/home/jswainston/Downloads/April2022CrossrefPublicDataFile/0.json.gz\", 'r') as fin:        \n",
    "    json_bytes = fin.read()                      \n",
    "\n",
    "json_str = json_bytes.decode('utf-8')            \n",
    "data = json.loads(json_str)  \n",
    "\n",
    "\n",
    "for item in data[\"items\"]:\n",
    "    print(\"DOI \" + item[\"DOI\"])\n",
    "    if \"author\" in item:\n",
    "        for author in item[\"author\"]:\n",
    "            for affiliation in author[\"affiliation\"]:\n",
    "                print(affiliation)\n",
    "                affiliationCountries = de.matchCountriesInAffiliation(affiliation[\"name\"])\n",
    "                print(affiliationCountries)\n",
    "    break \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "To extract and standardise the countries used in affiliations I first needed a standard list of countries. I manually created the COUNTRIES dictionary in the constants module using the standard ISO 3166 short country names and alpha-3 codes. This was then used as a lookup so that affiliation strings could be seached against the country names. I also extended the list of ISO countries to be able to group regions. For example, England, Scotland, Wales and Northern Ireland all map  to the same country code 'GBR'. Mapping them to the same country code means they can all be transformed to have the same name. In this example the are transformed to United Kingdom. This is important as in the data from the World Bank GDP is stored at the level of the United Kindgom and not the constituent countries that it is made up from. Doing this cleansing and transformation means that the statistics from the Crossref dataset can be aggregated and grouped in the same way as the World Bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The data model I have created is a multi-dimensional snowflake structure. I chose to use the dimensional model to make it simple for ease of use and for fast data retrieval through reducing the number of joins. The primary focus of the model is being able to count the number of author affiliations per country and the GDP per country. I have introduced an element of normalisation by having seperate entities for author and article. This is so that we don't have redundant article data stored against each author and makes it easier to calculate statistics about the articles. \n",
    "\n",
    "![title](udacityCapstoneDataModel.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1) Firstly data is downloaded from the sources described in the data gathering section\n",
    "2) The country and year dimensions are created first as they are static data sources. They also need to be in place for the fact tables to reference them. \n",
    "3) The fact_gdp table is then loaded from the World Bank csv file. \n",
    "4) The Crossref JSON files are then processed to create the author and article dimensions and the author_affiliation fact table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All of the code use to create the data model can be found in the files; etl.py, sqlQueries.py,createTables.py, constants.py and dataExtractUtils.py. To run the pipeline the folling commands can be issued in the following order:\n",
    "1) python3 createTables.py \n",
    "2) python3 etl.py\n",
    "\n",
    "#TODO add some pictures of resulting tables and some info about DB requirements and connection configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0       93416                93416\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\"\"\"\"\n",
    "Check for unique primary keys in each table. \n",
    "Compare total row count with count of distinct identifiers to check that they match\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=udacityprojectdb user=student password=6GNjBQvF\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "dim_article_primary_key_check = \"\"\"select \n",
    "count (article_id) as total_rows, count (distinct article_id) as unique_primary_keys\n",
    "from articles\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(dim_article_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0      476522               476522\n"
     ]
    }
   ],
   "source": [
    "fact_author_affiliation_primary_key_check = \"\"\"select \n",
    "count (affiliation_id) as total_rows, count (distinct affiliation_id) as unique_primary_keys\n",
    "from author_affiliation\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(fact_author_affiliation_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0     3026041              3026041\n"
     ]
    }
   ],
   "source": [
    "dim_author_primary_key_check = \"\"\"select \n",
    "count (author_id) as total_rows, count (distinct author_id) as unique_primary_keys\n",
    "from authors\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(dim_author_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0         247                  247\n"
     ]
    }
   ],
   "source": [
    "dim_country_primary_key_check = \"\"\"select \n",
    "count (country_id) as total_rows, count (distinct country_id) as unique_primary_keys\n",
    "from dim_country\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(dim_country_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0         401                  401\n"
     ]
    }
   ],
   "source": [
    "dim_year_primary_key_check = \"\"\"select \n",
    "count (year_id) as total_rows, count (distinct year_id) as unique_primary_keys\n",
    "from dim_year\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(dim_year_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_rows  unique_primary_keys\n",
      "0        1488                 1488\n"
     ]
    }
   ],
   "source": [
    "fact_gdp_primary_key_check = \"\"\"select \n",
    "count (id) as total_rows, count (distinct id) as unique_primary_keys\n",
    "from fact_gdp\n",
    "\"\"\"\n",
    "\n",
    "result = sqlio.read_sql_query(fact_gdp_primary_key_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for foreign key constraint. \n",
    "\"\"\"\n",
    "To check if foreign key constraints have been implemented correctly\n",
    "we will check to see if it's possible to delete a row from\n",
    "dim_country that is referenced from fact_author_affiliation. \n",
    "We expect PostgreSQL to throw an error about the foreign key\n",
    "constraint being violated\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\ndelete from dim_country\nwhere country_id = 77\n': update or delete on table \"dim_country\" violates foreign key constraint \"fk_country\" on table \"author_affiliation\"\nDETAIL:  Key (country_id)=(77) is still referenced from table \"author_affiliation\".\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForeignKeyViolation\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/sql.py:2020\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2020\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2021\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mForeignKeyViolation\u001b[0m: update or delete on table \"dim_country\" violates foreign key constraint \"fk_country\" on table \"author_affiliation\"\nDETAIL:  Key (country_id)=(77) is still referenced from table \"author_affiliation\".\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m fk_country_constraint_check \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdelete from dim_country\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwhere country_id = 77\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m result \u001b[39m=\u001b[39m sqlio\u001b[39m.\u001b[39;49mread_sql_query(fk_country_constraint_check, conn)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jswainston/udacity-data-engineering-capstone/src/CapstoneProjectTemplate.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/sql.py:399\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    398\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 399\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    400\u001b[0m     sql,\n\u001b[1;32m    401\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    402\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    403\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    404\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    405\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    406\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    407\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/sql.py:2080\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[1;32m   2069\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2077\u001b[0m ):\n\u001b[1;32m   2079\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2080\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   2081\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[1;32m   2083\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/sql.py:2032\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\ndelete from dim_country\nwhere country_id = 77\n': update or delete on table \"dim_country\" violates foreign key constraint \"fk_country\" on table \"author_affiliation\"\nDETAIL:  Key (country_id)=(77) is still referenced from table \"author_affiliation\".\n"
     ]
    }
   ],
   "source": [
    "fk_country_constraint_check = \"\"\"\n",
    "delete from dim_country\n",
    "where country_id = 77\n",
    "\"\"\"\n",
    "result = sqlio.read_sql_query(fk_country_constraint_check, conn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table | Field_Name | Field_Meaning | Field_Source\n",
    "-----|-----|-----|---- \n",
    "dim_article|article_id|Unique identifier for an article| Generated by etl for each article found in Crossref public data file\n",
    "dim_article|doi|Digital Object Identifier - a persistent unique identifier for digital objects. In this case journal articles| Crossref public data file\n",
    "dim_article|title|The title of the journal article | Crossref public data file\n",
    "dim_article|published_date|The date that the article was published | Crossref public data file\n",
    "dim_article|year_id|Foreign key to dim_year giving the year that the article was published | derived from Crossref public data file and dim_year\n",
    "dim_author|author_id|Unique identifier for an author | Generated by etl for each author found in Crossref public data file\n",
    "dim_author|article_id|Forgien key to article that the author published | Generated by etl\n",
    "dim_author|first_name|First name of an author of the article | Crossref public data file\n",
    "dim_author|last_name|Last name of an author of the article|Crossref public data file\n",
    "dim_country|country_id|Unique identifier for a country| Generated by etl for each country in ISO 3166 plus additions made for data quality \n",
    "dim_country|country_code|ISO 3166 Alpha-3 code|ISO 3166\n",
    "dim_country|country_name|English short country name|ISO 3166\n",
    "dim_year|year_id|Unique identifier for a year. Years in dimension range from 1800 to 2200|Generated by etl\n",
    "dim_year|year|Year in integer format|Generated by etl\n",
    "fact_author_affiliation|affiliation_id|unique identifier for an author affiliation|Generated by etl\n",
    "fact_author_affiliation|author_id|Foreign key to author afilliation with an institution|Derived from Crossref public data file and dim_author\n",
    "fact_author_affiliation|country_id|Forieng key to the country that the authors institution is located in | Derived from Crossref public data file and dim_country\n",
    "fact_gdp|gdp_id|Unique identifier for each GDP fact|Generated by etl\n",
    "fact_gdp|country_id|Foreign key to the country that the GDP fact is about|Derived from World Bank GDP file and dim_country\n",
    "fact_gdp|year_id|Foreign key to the year that the GDP fact is from|Derived from World Bank GDP file and dim_year\n",
    "fact_gdp|amount|The dollar amount of Gross Domestic Product Generated by a country in a year|World Bank GDP file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "**Clearly state the rationale for the choice of tools and technologies for the project**\n",
    "  \n",
    "I chose to use the Python programming language as the primary technology for wrting the ETL code. This is due to the large number of packages that support data development such as pandas, numpy and psycopg2. Being a high level language it also makes development more rapid than other languages such as Java. \n",
    "\n",
    "I chose to use a local PostgreSQL database for the project. PostgreSQL is a free and open-source relational database management system. With it being free it didn't add any cost to the project. A relational database suited the project due to the shape of the input data and the target model being a star schema for ease and speed of querying. \n",
    "\n",
    "A Jupyter notebook was used to show test results as it is a fantastic tool for communicating how code works with being able to blend text editing, code and code output.\n",
    "\n",
    "**Propose how often the data should be updated and why.**\n",
    "\n",
    "*Crossref* - After an initial load of the crossref publishing metadata could I would propose retrieving new records from the API on a daily basis. New content is being bublished all the time so it would be useful to have a daily view. Having said that the primary purpose of the anlysis is looking at macro trends so even though content may be published at ant time there isn't really any need for a real time view of this. \n",
    "\n",
    "*World Bank GDP data*- the World Bank GDP data is reported on an annual basis so new data would needed to be loaded each year when it becomes available\n",
    "\n",
    "<br>\n",
    "\n",
    "**Write a description of how you would approach the problem differently under the following scenarios:**\n",
    "\n",
    " * The data was increased by 100x.\n",
    "\n",
    "If the data was increased by 100x then I would use a scalable cloud database that supported both distributed storage and processing. A possible solution could be Amazon Redshift.  \n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "If the data needed to be available at a certain time each day then I would make sure that I used an orchestration tool with scheduling features. One possible solution would be Apache Airflow \n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    "  \n",
    " If the database neededd to be accessed by 100+ people I would choose a technology that was easily accessible and supported lots of concurrent connections. One possible option would be Amazon Redshift, a cloud data warehouse that promotes limitless concurrency.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
