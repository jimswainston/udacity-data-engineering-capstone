{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is an exploratory project to understand how metadata produced from the production of academic articles could be modelled and stored using a star schema design to enable efficient querying for analytics purposes.  The primary data source is the Crossref Public Data file (https://www.crossref.org/blog/2022-public-data-file-of-more-than-134-million-metadata-records-now-available/) which is a collection of more than 134 million metadata records. These records are produced from metadata submitted to Crossref by academic publishers as part of their publication process.  \n",
    "\n",
    "Additionally I have integrated GDP (gross domestic product) data to allow end users to try an answer questions such as how macroeconomic factors may influence the publishing environment. For example, is there a correlation between high GDP output and the number of academic documents produced?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I have written an ETL program that extracts information from journal article metadata and loads them into a star schema style database. In addition I have also loaded GDP data. The data from both sources has been integrated by using conformed dimensions for year and country. This allows end users to join facts about academic articles to GDP amounts. \n",
    "For the purposes of this project I have used a locally hosted PostgreSQL database, a Jupyter notebook and Python as the programming language of choice. \n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "The Crossref Public Data file which is the source of this data was downloaded from here: https://academictorrents.com/details/4dcfdf804775f2d92b7a030305fa0350ebef6f3e\n",
    "The full dataset includes 26,810 compressed JSON files. An example of one of these JSON files can be viewed here: https://raw.githubusercontent.com/jimswainston/udacity-data-engineering-capstone/main/data/Crossref/0.json. The type of information that is included is when a document was published, who it was published by, who the authors are, which institutions the authors are affiliated with at time of publication, the subject areas that the content covers and information about the journal that the article is contained in.\n",
    "\n",
    "The GDP data was sourced from the World Bank https://data.worldbank.org/indicator/NY.GDP.MKTP.CD. It is a CSV file that contains tabular data on the US$ GDP amounts for all countries that it keeps records for. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "One of the key pieces of information of interest is who authored a paper and the institution they were affiliated with. The institutional affiliation usually includes the country which allows us to understand the geography of where academic research is being produced. Unfortunately the affiliation isn't always present. See below JSON snippet for example. Missing affiliations need to be handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"author\": [\n",
    "        {\n",
    "          \"given\": \"Richard\",\n",
    "          \"family\": \"Arkaifie\",\n",
    "          \"sequence\": \"first\",\n",
    "          \"affiliation\": []\n",
    "        }\n",
    "      ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another core issue is that no standard taxonomy is used for entering country information for an affiliation. The affiliation is typically provided by the document author when they submit their article to a publisher. You could have different values being used to refer to the same geoographic region. For example, USA and North America. The other issue with the country information is that it is unstructured and is a free text part of the affiliation. See the below snippet as an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"author\": [\n",
    "        {\n",
    "          \"given\": \"Dale D.\",\n",
    "          \"family\": \"Tang\",\n",
    "          \"sequence\": \"first\",\n",
    "          \"affiliation\": [\n",
    "            {\n",
    "              \"name\": \"Department of Cellular and Integrative Physiology, Indiana University School of Medicine, 635 Barnhill Drive, Indianapolis, IN 46202, USA\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI 10.1149/1.1392467\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n",
      "{'name': 'Dipartimento di Chimica Fisica Applicata‐Politecnico di Milano, 20131 Milano, Italy'}\n",
      "['Italy']\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "#Country information is extracted and cleansed using the following Python method that is found in the dataExtractUtils.py module\n",
    "#This is an example of it working\n",
    "\n",
    "import pandas as pd\n",
    "import findspark\n",
    "import gzip\n",
    "import json\n",
    "import dataExtractUtils as de\n",
    "import uuid\n",
    "import constants\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "with gzip.open(\"/home/jswainston/Downloads/April2022CrossrefPublicDataFile/0.json.gz\", 'r') as fin:        \n",
    "    json_bytes = fin.read()                      \n",
    "\n",
    "json_str = json_bytes.decode('utf-8')            \n",
    "data = json.loads(json_str)  \n",
    "\n",
    "\n",
    "for item in data[\"items\"]:\n",
    "    print(\"DOI \" + item[\"DOI\"])\n",
    "    if \"author\" in item:\n",
    "        for author in item[\"author\"]:\n",
    "            for affiliation in author[\"affiliation\"]:\n",
    "                print(affiliation)\n",
    "                affiliationCountries = de.match_countries_in_affiliation(affiliation[\"name\"])\n",
    "                print(affiliationCountries)\n",
    "    break \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "To extract and standardise the countries used in affiliations I first needed a standard list of countries. I manually created the COUNTRIES dictionary in the constants module using the standard ISO 3166 short country names and alpha-3 codes. This was then used as a lookup so that affiliation strings could be seached against the country names. I also extended the list of ISO countries to be able to group regions. For example, England, Scotland, Wales and Northern Ireland all map  to the same country code 'GBR'. Mapping them to the same country code means they can all be transformed to have the same name. In this example the are transformed to United Kingdom. This is important as in the data from the World Bank GDP is stored at the level of the United Kindgom and not the constituent countries that it is made up from. Doing this cleansing and transformation means that the statistics from the Crossref dataset can be aggregated and grouped in the same way as the World Bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The data model I have created is a multi-dimensional snowflake structure. I chose to use the dimensional model to make it simple for ease of use and for fast data retrieval through reducing the number of joins. The primary focus of the model is being able to count the number of author affiliations per country and the GDP per country. I have introduced an element of normalisation by having seperate entities for author and article. This is so that we don't have redundant article data stored against each author and makes it easier to calculate statistics about the articles. \n",
    "\n",
    "![title](udacityCapstoneDataModel.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1) Firstly data is downloaded from the sources described in the data gathering section\n",
    "2) The country and year dimensions are created first as they are static data sources. They also need to be in place for the fact tables to reference them. \n",
    "3) The fact_gdp table is then loaded from the World Bank csv file. \n",
    "4) The Crossref JSON files are then processed to create the author and article dimensions and the author_affiliation fact table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This project relies on a local PostgreSQL instance being installed on the users machine. Before running any of the exectutibles in the project PostgreSQL must be installed and the CONNECTION_STRING constant in the constants.py must be updated to enable connection to the installed database. It is just the user\n",
    "and password variables that need ammending. \n",
    "\n",
    "All of the code use to create the data model can be found in the files; etl.py, sqlQueries.py,createTables.py, constants.py and dataExtractUtils.py. To run the pipeline the folling commands can be issued in the following order:\n",
    "1) python3 createTables.py \n",
    "2) python3 etl.py\n",
    "\n",
    "After running these python files your database should be populated with the dables listed below with the number of rows stated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in articles is 788947\n",
      "Total number of records in author_affiliation is 282352\n",
      "Total number of records in authors is 2604264\n",
      "Total number of records in dim_country is 247\n",
      "Total number of records in dim_year is 401\n",
      "Total number of records in fact_gdp is 1488\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=udacityprojectdb user=student password=6GNjBQvF\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "articles_count= \"select count(article_id) from articles\"\n",
    "affiliations_count = \"select count(affiliation_id) from author_affiliation\"\n",
    "authors_count = \"select count(author_id) from authors\"\n",
    "countries_count = \"select count(country_id) from dim_country\"\n",
    "year_count = \"select count(year_id) from dim_year\"\n",
    "gdp_count = \"select count(id) from fact_gdp\"\n",
    "\n",
    "articles_result = sqlio.read_sql_query(articles_count, conn)\n",
    "affiliations_result = sqlio.read_sql_query(affiliations_count, conn)\n",
    "authors_result = sqlio.read_sql_query(authors_count, conn)\n",
    "country_result = sqlio.read_sql_query(countries_count, conn)\n",
    "year_result = sqlio.read_sql_query(year_count, conn)\n",
    "gdp_result = sqlio.read_sql_query(gdp_count, conn)\n",
    "\n",
    "print(\"Total number of records in articles is \" + str(articles_result.iloc[0]['count']))\n",
    "print(\"Total number of records in author_affiliation is \" + str(affiliations_result.iloc[0]['count']))\n",
    "print(\"Total number of records in authors is \" + str(authors_result.iloc[0]['count']))\n",
    "print(\"Total number of records in dim_country is \" + str(country_result.iloc[0]['count']))\n",
    "print(\"Total number of records in dim_year is \" + str(year_result.iloc[0]['count']))\n",
    "print(\"Total number of records in fact_gdp is \" + str(gdp_result.iloc[0]['count']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below table shows the number of author contributions per country in 2020\n",
      "and the total GDP for that year. The affiliation_counts will not be accurate\n",
      "as we have only processed a small subset of the full crossref database \n",
      "\n",
      "    affiliation_count   country_name    gdp_amount\n",
      "0                   1      Australia  1.327836e+12\n",
      "1                   3        Austria  4.332585e+11\n",
      "2                   1     Azerbaijan  4.269300e+10\n",
      "3                   1     Bangladesh  3.739021e+11\n",
      "4                   1         Canada  1.645423e+12\n",
      "5                   2          China  1.468767e+13\n",
      "6                   1        Germany  3.846414e+12\n",
      "7                   6        Denmark  3.560849e+11\n",
      "8                   1        Algeria  1.450092e+11\n",
      "9                  20          Spain  1.281485e+12\n",
      "10                  3        Finland  2.718370e+11\n",
      "11                  1         France  2.630318e+12\n",
      "12                  4        Georgia  1.584292e+10\n",
      "13                 78      Indonesia  1.058689e+12\n",
      "14                 18          India  2.667688e+12\n",
      "15                 16           Iran  2.315476e+11\n",
      "16                  1         Israel  4.071007e+11\n",
      "17                  9         Jersey           NaN\n",
      "18                 62          Japan  5.040108e+12\n",
      "19                123          Korea  1.637896e+12\n",
      "20                  1         Mexico  1.087118e+12\n",
      "21                  1           Mali  1.746539e+10\n",
      "22                  3          Burma  7.893026e+10\n",
      "23                 27       Malaysia  3.370060e+11\n",
      "24                  5       Pakistan  3.003063e+11\n",
      "25                  8          Qatar  1.444114e+11\n",
      "26                  7         Russia  1.488322e+12\n",
      "27                  7   Saudi Arabia  7.033678e+11\n",
      "28                  1         Taiwan           NaN\n",
      "29                  2  United States  2.089374e+13\n",
      "30                  1        Vietnam  3.432426e+11\n",
      "31                  1          Yemen  1.884051e+10\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=udacityprojectdb user=student password=6GNjBQvF\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# This example of a query against the resulting database shows how the two datasets have been integrated around geographic temporal data. \n",
    "# GDP and journal article publishing data can be combined for different years and countries. \n",
    "\n",
    "integrated_query= \"\"\"\n",
    "select count(author_affiliation.affiliation_id) as affiliation_count, dim_country.country_name,gdp_amount\n",
    "from authors\n",
    "inner join articles\n",
    "on articles.article_id = authors.article_id\n",
    "inner join author_affiliation\n",
    "on author_affiliation.author_id = authors.author_id\n",
    "inner join dim_country \n",
    "ON dim_country.country_id = author_affiliation.country_id\n",
    "inner join dim_year \n",
    "on dim_year.year_id = articles.year_id\n",
    "inner join fact_gdp ON fact_gdp.year_id = dim_year.year_id AND fact_gdp.country_id = dim_country.country_id\n",
    "where dim_year.year = 2020\n",
    "group by dim_country.country_id,gdp_amount\n",
    "\"\"\"\n",
    "\n",
    "results = sqlio.read_sql_query(integrated_query, conn)\n",
    "\n",
    "print(\"The below table shows the number of author contributions per country in 2020\")\n",
    "print(\"and the total GDP for that year. The affiliation_counts will not be accurate\")\n",
    "print(\"as we have only processed a small subset of the full crossref database \\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table articles\n",
      "\n",
      "total_rows is 788947\n",
      "\n",
      "total_primary_keys is 788947\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\"\"\"\"\n",
    "Check for unique primary keys in each table. \n",
    "Compare total row count with count of distinct identifiers to check that they match\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "import qualityCheckUtils as qc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=udacityprojectdb user=student password=6GNjBQvF\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "qc.primary_key_count_test(conn,\"articles\",\"article_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table author_affiliation\n",
      "\n",
      "total_rows is 282352\n",
      "\n",
      "total_primary_keys is 282352\n"
     ]
    }
   ],
   "source": [
    "qc.primary_key_count_test(conn,\"author_affiliation\",\"affiliation_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table authors\n",
      "\n",
      "total_rows is 2604264\n",
      "\n",
      "total_primary_keys is 2604264\n"
     ]
    }
   ],
   "source": [
    "qc.primary_key_count_test(conn,\"authors\",\"author_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table dim_country\n",
      "\n",
      "total_rows is 247\n",
      "\n",
      "total_primary_keys is 247\n"
     ]
    }
   ],
   "source": [
    "qc.primary_key_count_test(conn,\"dim_country\",\"country_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table dim_year\n",
      "\n",
      "total_rows is 401\n",
      "\n",
      "total_primary_keys is 401\n"
     ]
    }
   ],
   "source": [
    "qc.primary_key_count_test(conn,\"dim_year\",\"year_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key check passed for table fact_gdp\n",
      "\n",
      "total_rows is 1488\n",
      "\n",
      "total_primary_keys is 1488\n"
     ]
    }
   ],
   "source": [
    "qc.primary_key_count_test(conn,\"fact_gdp\",\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for foreign key constraint. \n",
    "\"\"\"\n",
    "To check if foreign key constraints have been implemented correctly\n",
    "we will check to see if it's possible to delete a row from\n",
    "dim_country that is referenced from fact_author_affiliation. \n",
    "We expect PostgreSQL to throw an error about the foreign key\n",
    "constraint being violated\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreign key constraint test passed. Database threw the correct error as shown below \n",
      "\n",
      "update or delete on table \"dim_country\" violates foreign key constraint \"fk_country\" on table \"author_affiliation\"\n",
      "DETAIL:  Key (country_id)=(77) is still referenced from table \"author_affiliation\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qc.foreign_key_constraint_test(conn,\"dim_country\",\"country_id\",77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table | Field_Name | Field_Meaning | Field_Source\n",
    "-----|-----|-----|---- \n",
    "dim_article|article_id|Unique identifier for an article| Generated by etl for each article found in Crossref public data file\n",
    "dim_article|doi|Digital Object Identifier - a persistent unique identifier for digital objects. In this case journal articles| Crossref public data file\n",
    "dim_article|title|The title of the journal article | Crossref public data file\n",
    "dim_article|published_date|The date that the article was published | Crossref public data file\n",
    "dim_article|year_id|Foreign key to dim_year giving the year that the article was published | derived from Crossref public data file and dim_year\n",
    "dim_author|author_id|Unique identifier for an author | Generated by etl for each author found in Crossref public data file\n",
    "dim_author|article_id|Forgien key to article that the author published | Generated by etl\n",
    "dim_author|first_name|First name of an author of the article | Crossref public data file\n",
    "dim_author|last_name|Last name of an author of the article|Crossref public data file\n",
    "dim_country|country_id|Unique identifier for a country| Generated by etl for each country in ISO 3166 plus additions made for data quality \n",
    "dim_country|country_code|ISO 3166 Alpha-3 code|ISO 3166\n",
    "dim_country|country_name|English short country name|ISO 3166\n",
    "dim_year|year_id|Unique identifier for a year. Years in dimension range from 1800 to 2200|Generated by etl\n",
    "dim_year|year|Year in integer format|Generated by etl\n",
    "fact_author_affiliation|affiliation_id|unique identifier for an author affiliation|Generated by etl\n",
    "fact_author_affiliation|author_id|Foreign key to author afilliation with an institution|Derived from Crossref public data file and dim_author\n",
    "fact_author_affiliation|country_id|Forieng key to the country that the authors institution is located in | Derived from Crossref public data file and dim_country\n",
    "fact_gdp|gdp_id|Unique identifier for each GDP fact|Generated by etl\n",
    "fact_gdp|country_id|Foreign key to the country that the GDP fact is about|Derived from World Bank GDP file and dim_country\n",
    "fact_gdp|year_id|Foreign key to the year that the GDP fact is from|Derived from World Bank GDP file and dim_year\n",
    "fact_gdp|amount|The dollar amount of Gross Domestic Product Generated by a country in a year|World Bank GDP file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "**Clearly state the rationale for the choice of tools and technologies for the project**\n",
    "  \n",
    "I chose to use the Python programming language as the primary technology for wrting the ETL code. This is due to the large number of packages that support data development such as pandas, numpy and psycopg2. Being a high level language it also makes development more rapid than other languages such as Java. \n",
    "\n",
    "I chose to use a local PostgreSQL database for the project. PostgreSQL is a free and open-source relational database management system. With it being free it didn't add any cost to the project. A relational database suited the project due to the shape of the input data and the target model being a star schema for ease and speed of querying. \n",
    "\n",
    "A Jupyter notebook was used to show test results as it is a fantastic tool for communicating how code works with being able to blend text editing, code and code output.\n",
    "\n",
    "**Propose how often the data should be updated and why.**\n",
    "\n",
    "*Crossref* - After an initial load of the crossref publishing metadata could I would propose retrieving new records from the API on a daily basis. New content is being bublished all the time so it would be useful to have a daily view. Having said that the primary purpose of the anlysis is looking at macro trends so even though content may be published at ant time there isn't really any need for a real time view of this. \n",
    "\n",
    "*World Bank GDP data*- the World Bank GDP data is reported on an annual basis so new data would needed to be loaded each year when it becomes available\n",
    "\n",
    "<br>\n",
    "\n",
    "**Write a description of how you would approach the problem differently under the following scenarios:**\n",
    "\n",
    " * The data was increased by 100x.\n",
    "\n",
    "If the data was increased by 100x then I would use a scalable cloud database that supported both distributed storage and processing. A possible solution could be Amazon Redshift.  \n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "If the data needed to be available at a certain time each day then I would make sure that I used an orchestration tool with scheduling features. One possible solution would be Apache Airflow \n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    "  \n",
    " If the database neededd to be accessed by 100+ people I would choose a technology that was easily accessible and supported lots of concurrent connections. One possible option would be Amazon Redshift, a cloud data warehouse that promotes limitless concurrency.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
